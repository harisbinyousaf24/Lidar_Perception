{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3e8b11",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "87cc1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyntcloud import PyntCloud\n",
    "from tqdm import tqdm\n",
    "from rosbags.highlevel import AnyReader\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import subprocess\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import utm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ae06a",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7befd",
   "metadata": {},
   "source": [
    "### Extractor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "339bf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bag_file(input_bag_path, output_dir, topics, save_as='pcd'):\n",
    "    \n",
    "    if not os.path.exists(input_bag_path):\n",
    "        print(f\"Error: The file {input_bag_path} does not exist.\")\n",
    "        return\n",
    "        \n",
    "    data_dir = os.path.join(output_dir, 'data')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    lidar_frames = os.path.join(data_dir, 'frames')\n",
    "    os.makedirs(lidar_frames, exist_ok=True)\n",
    "    \n",
    "    gps_ts, latitudes, longitudes, altitudes = [], [], [], []\n",
    "    \n",
    "    with AnyReader([Path(input_bag_path)]) as reader:\n",
    "        connections = [x for x in reader.connections if x.topic in topics]\n",
    "\n",
    "        for connection, timestamp, rawdata in tqdm(reader.messages(connections=connections), desc='Processing Bag...'):\n",
    "            topic_name = connection.topic\n",
    "\n",
    "            if topic_name == '/rslidar_points':\n",
    "                msg = reader.deserialize(rawdata, connection.msgtype)\n",
    "                points_data = np.frombuffer(msg.data, dtype=np.float32).reshape((msg.height, msg.width, 4))  # Assuming x, y, z, intensity\n",
    "                \n",
    "                pcd = points_data[:, :, :3]\n",
    "                intensities = points_data[:, :, 3]\n",
    "                flat_pcd = pcd.reshape(-1, 3)\n",
    "                flat_intensities = intensities.reshape(-1, 1)\n",
    "                \n",
    "                # Save to PCD\n",
    "                df = pd.DataFrame(data=np.hstack((flat_pcd, flat_intensities)), columns=['x', 'y', 'z', 'intensity'])\n",
    "                cloud = PyntCloud(df)\n",
    "                ply_file = os.path.join(lidar_frames, f\"{timestamp}.ply\")\n",
    "                \n",
    "                if save_as == 'pcd':\n",
    "                    pcd_file = os.path.join(lidar_frames, f\"{timestamp}.pcd\")\n",
    "                    ply_to_pcd(ply_file, pcd_file)\n",
    "                    os.remove(ply_file)\n",
    "                    \n",
    "                elif save_as == 'ply':\n",
    "                    cloud.to_file(ply_file)\n",
    "                \n",
    "            elif topic_name == '/gnss':\n",
    "                msg = reader.deserialize(rawdata, connection.msgtype)\n",
    "                gps_ts.append(timestamp)\n",
    "                latitudes.append(msg.latitude)\n",
    "                longitudes.append(msg.longitude)\n",
    "                altitudes.append(msg.altitude)\n",
    "\n",
    "\n",
    "    # GPS data to JSON\n",
    "    gps_data = {\n",
    "        \"timestamps\": gps_ts,\n",
    "        \"latitude\": latitudes,\n",
    "        \"longitude\": longitudes,\n",
    "        \"altitude\": altitudes\n",
    "    }\n",
    "    gps_json_path = os.path.join(data_dir, \"gnss.json\")\n",
    "    with open(gps_json_path, 'w') as f:\n",
    "        json.dump(gps_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Lidar frames saved in directory: {lidar_frames}\")\n",
    "    print(f\"GPS data saved at: {gps_json_path}\")\n",
    "    \n",
    "    \n",
    "    return gps_json_path, lidar_frames\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5adbbb",
   "metadata": {},
   "source": [
    "### PLY to PCD Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "93e7bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ply_to_pcd(ply_file, pcd_file):\n",
    "    # Read the PLY file data\n",
    "    ply_data = PlyData.read(ply_file)\n",
    "    vertex_data = ply_data['vertex'].data\n",
    "\n",
    "    # Extract coordinate and intensity data\n",
    "    x = vertex_data['x']\n",
    "    y = vertex_data['y']\n",
    "    z = vertex_data['z']\n",
    "    intensity = vertex_data['intensity']\n",
    "\n",
    "    # Stack these arrays into a single numpy array\n",
    "    data = np.stack((x, y, z, intensity), axis=-1)\n",
    "    num_points = data.shape[0]\n",
    "\n",
    "    # Prepare the PCD file header\n",
    "    header = f\"\"\"# .PCD v0.7 - Point Cloud Data file format\n",
    "VERSION 0.7\n",
    "FIELDS x y z intensity\n",
    "SIZE 4 4 4 4\n",
    "TYPE F F F F\n",
    "COUNT 1 1 1 1\n",
    "WIDTH {num_points}\n",
    "HEIGHT 1\n",
    "VIEWPOINT 0 0 0 1 0 0 0\n",
    "POINTS {num_points}\n",
    "DATA ascii\n",
    "\"\"\"\n",
    "\n",
    "    # Write the header and data to the PCD file\n",
    "    with open(pcd_file, 'w') as f:\n",
    "        f.write(header)\n",
    "        np.savetxt(f, data, fmt='%f %f %f %f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca89d58",
   "metadata": {},
   "source": [
    "### Input Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "39065145",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bag = '/home/ty/Downloads/AAI/Bags/hamburg.bag'\n",
    "if os.path.exists(input_bag):\n",
    "    bag_name = os.path.splitext(os.path.basename(input_bag))[0]\n",
    "    main_dir = os.path.join(os.path.dirname(input_bag), bag_name)\n",
    "    os.makedirs(main_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "29743627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Bag...: 266it [00:02, 104.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidar frames saved in directory: /home/ty/Downloads/AAI/Bags/hamburg/data/frames\n",
      "GPS data saved at: /home/ty/Downloads/AAI/Bags/hamburg/data/gnss.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gps_data, frames_dir = read_bag_file(input_bag, main_dir,\n",
    "                                     ['/rslidar_points', '/gnss'],\n",
    "                                    save_as='ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976a94a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f8eed795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_point_cloud(file_or_cloud):\n",
    "    if isinstance(file_or_cloud, PyntCloud):\n",
    "        return file_or_cloud\n",
    "    elif isinstance(file_or_cloud, str):\n",
    "        cloud = PyntCloud.from_file(file_or_cloud)\n",
    "        return cloud\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a file path (str) or a Point Cloud object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38251a05",
   "metadata": {},
   "source": [
    "### Playground Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "528735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyntcloud_to_open3d(file_or_cloud):\n",
    "    cloud = read_point_cloud(file_or_cloud)\n",
    "    cloud_pts = cloud.points[['x', 'y', 'z', 'intensity']].values\n",
    "    \n",
    "    xyz = cloud_pts[:, :3]\n",
    "    open3d_cloud = o3d.geometry.PointCloud()\n",
    "    open3d_cloud.points = o3d.utility.Vector3dVector(xyz)\n",
    "    return open3d_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b376c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_pcd(file, clr='r'):\n",
    "    cloud = read_point_cloud(file)\n",
    "    pcd = pyntcloud_to_open3d(cloud)\n",
    "    \n",
    "    if clr == 'r':\n",
    "        color = [1, 0, 0]\n",
    "        pcd.paint_uniform_color(color)\n",
    "    elif clr == 'g':\n",
    "        color = [0, 1, 0]\n",
    "        pcd.paint_uniform_color(color)\n",
    "    elif clr == 'b':\n",
    "        color = [0, 0, 1]\n",
    "        pcd.paint_uniform_color(color)\n",
    "        \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f0a8e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pcd(pcd):\n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b6e4008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_pcds(*pcds):\n",
    "    clouds = [pcd for pcd in pcds]\n",
    "    o3d.visualization.draw_geometries(clouds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1fd8",
   "metadata": {},
   "source": [
    "### Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "393f6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(file):\n",
    "    pcd = read_point_cloud(file)\n",
    "    points_df = pcd.points\n",
    "    points_df.dropna(subset=['x', 'y', 'z'], inplace=True)\n",
    "    return PyntCloud(points_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9f56d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ego(file, POG_z=-2.9):\n",
    "    pcd = read_point_cloud(file)\n",
    "    points_df = pcd.points\n",
    "    points = points_df.values\n",
    "    # EGO Dimensions\n",
    "    ego_length, ego_width, ego_height = 4.27, 2.02, 1.45\n",
    "\n",
    "    lidar_from_front = ego_length * 0.62\n",
    "    lidar_from_rear = ego_length - lidar_from_front\n",
    "    lidar_from_right = ego_width / 2\n",
    "    lidar_roof_clearance = POG_z + ego_height\n",
    "\n",
    "    # Masking\n",
    "    points = np.asarray(pcd.points)\n",
    "    x_mask = np.logical_or(points[:, 0] < -lidar_from_rear, \n",
    "                           points[:, 0] > lidar_from_front)\n",
    "    y_mask = np.logical_or(points[:, 1] < -(lidar_from_right),\n",
    "                          points[:, 1] > lidar_from_right)\n",
    "    z_mask = np.logical_or(points[:, 2] < POG_z,\n",
    "                          points[:, 2] > 0)\n",
    "    \n",
    "    ego_mask = np.logical_or.reduce((x_mask, y_mask, z_mask))\n",
    "\n",
    "    # Filtering points using the mask and keeping the intensity\n",
    "    inlier_points = points_df[ego_mask]\n",
    "    outlier_points = points_df[~ego_mask]\n",
    "\n",
    "    # Convert filtered DataFrames back to PyntClouds, including intensity data\n",
    "    inlier_cloud = PyntCloud(inlier_points)\n",
    "    outlier_cloud = PyntCloud(outlier_points)\n",
    "\n",
    "    return inlier_cloud, outlier_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "31499e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_basic_preprocessing(input_dir, output_dir, filter_ego=True):\n",
    "    sorted_frames = sorted([os.path.join(input_dir, frame) for frame in os.listdir(input_dir) if frame.endswith(\".ply\")])\n",
    "    for file in tqdm(sorted_frames, desc='Basic Preprocessing', total=len(sorted_frames)):\n",
    "        name = os.path.splitext(os.path.basename(file))[0]\n",
    "        processed_file = os.path.join(output_dir, name+'.ply')\n",
    "        clean_cloud = remove_nans(file)\n",
    "        if filter_ego:\n",
    "            filtered_cloud, _ = remove_ego(clean_cloud)\n",
    "            filtered_cloud.to_file(processed_file)\n",
    "        else:\n",
    "            clean_cloud.to_file(processed_file)\n",
    "    print(f\"Basic Preprocessing Completed! Frames dumped at {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d26f2",
   "metadata": {},
   "source": [
    "### Advanced Preprocessing (Noise Filtration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a13e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_outlier_removal(file, k_nbs, z_thresh):\n",
    "    cloud = read_point_cloud(file)\n",
    "    # Extract only the spatial coordinates for KDTree calculations\n",
    "    spatial_points = np.array(cloud.points[['x', 'y', 'z']])\n",
    "\n",
    "    # Create a KDTree for efficient nearest neighbor search\n",
    "    kdtree = cKDTree(spatial_points)\n",
    "    distances, _ = kdtree.query(spatial_points, k=k_nbs + 1)\n",
    "\n",
    "    # Compute mean and standard deviation of distances to k nearest neighbors\n",
    "    mean_distances = np.mean(distances[:, 1:], axis=1)  # exclude the first distance (self)\n",
    "    std_dev = np.std(mean_distances)\n",
    "    mean_of_mean_distances = np.mean(mean_distances)\n",
    "\n",
    "    # Filter points based on the mean distance within standard deviation threshold\n",
    "    mask = mean_distances <= (mean_of_mean_distances + z_thresh * std_dev)\n",
    "    outlier_mask = mean_distances > (mean_of_mean_distances + z_thresh * std_dev)\n",
    "\n",
    "    # Use masks to separate inliers and outliers, including all columns\n",
    "    inlier_points_df = cloud.points[mask]\n",
    "    outlier_points_df = cloud.points[outlier_mask]\n",
    "\n",
    "    inlier_cloud = PyntCloud(inlier_points_df)\n",
    "    outlier_cloud = PyntCloud(outlier_points_df)\n",
    "    \n",
    "    return inlier_cloud, outlier_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6b67cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_filter(file, z_range):\n",
    "    z_min, z_max = z_range\n",
    "    pcd = read_point_cloud(file)\n",
    "    points_df = pcd.points\n",
    "    points = points_df.values\n",
    "    \n",
    "    z_axis = points[:, 2]\n",
    "    z_mask = np.logical_and(z_axis >= z_min, z_axis <= z_max)\n",
    "    \n",
    "    # Filtering inlier points based on the mask\n",
    "    inlier_points_df = points_df[z_mask]\n",
    "    outlier_points_df = points_df[~z_mask]\n",
    "    \n",
    "    # Convert filtered DataFrames back to PyntClouds\n",
    "    inlier_cloud = PyntCloud(inlier_points_df)\n",
    "    outlier_cloud = PyntCloud(outlier_points_df)\n",
    "    \n",
    "    return inlier_cloud, outlier_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e79ce5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_advanced_preprocessing(input_dir, \n",
    "                                 params, apply_SOR=True, \n",
    "                                 apply_z_filtering=True):\n",
    "    k_nbs, thresh = params['SOR']\n",
    "    z_min, z_max = params['z_filter']\n",
    "    \n",
    "    sorted_frames = sorted([os.path.join(input_dir, frame) for frame in os.listdir(input_dir) if frame.endswith(\".ply\")])\n",
    "    \n",
    "    for file in tqdm(sorted_frames, desc='Advanced Preprocessing', total=len(sorted_frames)):\n",
    "        if apply_SOR:\n",
    "            filtered_cloud, _ = statistical_outlier_removal(file, k_nbs, thresh)\n",
    "            \n",
    "        if apply_z_filtering:\n",
    "            filtered_cloud, _ = z_filter(filtered_cloud, [z_min, z_max])\n",
    "            \n",
    "        filtered_cloud.to_file(file)\n",
    "        \n",
    "    print(f\"Advanced Preprocessing Completed! Frames dumped at {input_dir}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8ea4bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = True\n",
    "basic_preprocessing = True\n",
    "advanced_preprocessing = False\n",
    "advanced_params = {'SOR':[10, 3],\n",
    "                   'z_filter':[-2.9, 8]}\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3e80de6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Basic Preprocessing: 100%|████████████████████| 190/190 [00:02<00:00, 66.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Preprocessing Completed! Frames dumped at /home/ty/Downloads/AAI/Bags/hamburg/data/preprocessed_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if preprocess:\n",
    "    data_dir = os.path.dirname(frames_dir)\n",
    "    preprocessed_frames = os.path.join(data_dir, 'preprocessed_frames')\n",
    "    os.makedirs(preprocessed_frames, exist_ok=True)\n",
    "    \n",
    "    if basic_preprocessing:\n",
    "        apply_basic_preprocessing(frames_dir, preprocessed_frames, filter_ego=True)\n",
    "        basic_preprocessing = False\n",
    "    if advanced_preprocessing:\n",
    "        apply_advanced_preprocessing(preprocessed_frames,\n",
    "                                    advanced_params, apply_SOR=True,\n",
    "                                    apply_z_filtering=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82303656",
   "metadata": {},
   "source": [
    "# Lidar Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "31cbe783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kiss_icp_pipeline(directory_path, output_directory, parameters, apply_parameters=False):\n",
    "    \n",
    "    maxRange = str(parameters['maxRange'])\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    output_dir = os.path.join(output_directory, 'kiss_icp_results')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    os.chdir(output_dir)\n",
    "    if apply_parameters:\n",
    "        command = [\n",
    "            \"kiss_icp_pipeline\",\n",
    "            directory_path,\n",
    "            '--max_range',\n",
    "            maxRange,\n",
    "            '--deskew'\n",
    "        ]\n",
    "        print(f\"Using command {command} with settings:\\n {parameters}\\n apply_parameters: {apply_parameters}\")\n",
    "    \n",
    "    else:\n",
    "        command = [\n",
    "            \"kiss_icp_pipeline\",\n",
    "            directory_path\n",
    "        ]\n",
    "        print(f\"Using command {command} with settings:\\n {parameters}\\n apply_parameters: {apply_parameters}\")\n",
    "    try:\n",
    "        subprocess.run(command)\n",
    "    except Exception as e:\n",
    "        print(f\"Error Running kiss_icp_pipeline: {e}\")\n",
    "    os.chdir(current_dir)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ef4ec3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_odometry = True\n",
    "maxRange = 150\n",
    "\n",
    "odom_params = {'maxRange':maxRange}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6310d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using command ['kiss_icp_pipeline', '/home/ty/Downloads/AAI/Bags/hamburg/data/preprocessed_frames', '--max_range', '150', '--deskew'] with settings:\n",
      " {'maxRange': 150}\n",
      " apply_parameters: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "-- Install configuration: \"Release\"\n",
      "-- Up-to-date: /home/ty/anaconda3/envs/kiss-icp-env/lib/python3.8/site-packages/./kiss_icp_pybind.cpython-38-x86_64-linux-gnu.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cmake --build & --install in /home/ty/kiss-icp/python/build/cp38-cp38-manylinux_2_31_x86_64\n",
      "Dataloader Type: GenericDataset\n",
      "Trying to guess how to read your data: `pip install \"kiss-icp[all]\"` is required\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 190/190 [00:13<00:00, 14.34 frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses saved at /home/ty/Downloads/AAI/Bags/hamburg/odometry/kiss_icp_results/results/2024-06-29_08-24-03/preprocessed_frames_poses\n",
      "\n",
      "Runtime settings written at: /home/ty/Downloads/AAI/Bags/hamburg/odometry/kiss_icp_results/results/2024-06-29_08-24-03/runtime_settings.yaml\n",
      " ─────────────────────────────────── \n",
      " \u001b[1m \u001b[0m\u001b[1m           Metric\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mValue\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mUnits\u001b[0m\u001b[1m \u001b[0m \n",
      " ─────────────────────────────────── \n",
      " \u001b[36m \u001b[0m\u001b[36mAverage Frequency\u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m 15  \u001b[0m\u001b[35m \u001b[0m \u001b[32m \u001b[0m\u001b[32mHz   \u001b[0m\u001b[32m \u001b[0m \n",
      " \u001b[36m \u001b[0m\u001b[36m  Average Runtime\u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m 68  \u001b[0m\u001b[35m \u001b[0m \u001b[32m \u001b[0m\u001b[32mms   \u001b[0m\u001b[32m \u001b[0m \n",
      " ─────────────────────────────────── \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if compute_odometry:\n",
    "    odom_dir = os.path.join(main_dir, 'odometry')\n",
    "    os.makedirs(odom_dir, exist_ok=True)\n",
    "    lidar_odom = run_kiss_icp_pipeline(preprocessed_frames,\n",
    "                                      odom_dir, odom_params,\n",
    "                                      apply_parameters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094a5ac",
   "metadata": {},
   "source": [
    "# Trajectory Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bda831d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_states(file):\n",
    "    data = np.load(file)\n",
    "    translations = []\n",
    "    for tf in data:\n",
    "        translation = tf[:3, 3]\n",
    "        translations.append(translation)\n",
    "        \n",
    "    return data, np.array(translations)\n",
    "\n",
    "def read_gps(file):\n",
    "    with open(file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    lat = np.array(data['latitude'])\n",
    "    lon = np.array(data['longitude'])\n",
    "    alt = np.array(data['altitude'])\n",
    "    \n",
    "    data = np.column_stack((lat, lon, alt))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "53ab5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_global_to_local(gps_data):\n",
    "    latlon_gps = gps_data[:, :2]\n",
    "    first_lat, first_lon = latlon_gps[0]\n",
    "    utm_x, utm_y, zone_num, zone_letter = utm.from_latlon(first_lat, first_lon)\n",
    "    offset = np.array([utm_x, utm_y])\n",
    "    utm_mat = []\n",
    "    for lat, lon in latlon_gps:\n",
    "        ux, uy, _, _ = utm.from_latlon(lat, lon)\n",
    "        utm_arr = np.array([ux, uy])\n",
    "        utm_mat.append(utm_arr)\n",
    "        \n",
    "    utm_mat = np.array(utm_mat)\n",
    "    local_mat = utm_mat - offset\n",
    "    off_lst = [offset, zone_num, zone_letter]\n",
    "    \n",
    "    return local_mat, off_lst\n",
    "\n",
    "def convert_local_to_global(local_data, offset):\n",
    "    utm_offset = offset[0]\n",
    "    zone_num = offset[1]\n",
    "    zone_letter = offset[2]\n",
    "    local_data = local_data[:, :2]\n",
    "    \n",
    "    global_mat = []\n",
    "    for x, y in local_data:\n",
    "        # Add the local offset to the UTM offset\n",
    "        ux = utm_offset[0] + x\n",
    "        uy = utm_offset[1] + y\n",
    "        lat, lon = utm.to_latlon(ux, uy, zone_num, zone_letter)\n",
    "        latlon_arr = np.array([lat, lon])\n",
    "        global_mat.append(latlon_arr)\n",
    "        \n",
    "    global_mat = np.array(global_mat)\n",
    "    \n",
    "    return global_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b009588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_matrix(heading_angle):\n",
    "    angle_radians = np.radians(heading_angle)\n",
    "    rotation_matrix = np.array([[np.cos(angle_radians), -np.sin(angle_radians), 0, 0],\n",
    "                                    [np.sin(angle_radians), np.cos(angle_radians), 0, 0],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 0, 1]])\n",
    "    return rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9c861646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotation_based_on_gps(gps_data, odom_data, frame_idx=-1, add_offset=0):\n",
    "    poses_data = odom_data[:, :2]\n",
    "    gps_data = gps_data[:, :2]\n",
    "    \n",
    "    tangent_gps = gps_data[frame_idx] - gps_data[0]\n",
    "    tangent_lidar = poses_data[frame_idx] - poses_data[0]\n",
    "\n",
    "    tangent_gps_norm = tangent_gps / np.linalg.norm(tangent_gps)\n",
    "    tangent_lidar_norm = tangent_lidar / np.linalg.norm(tangent_lidar)\n",
    "\n",
    "    dot_product = np.dot(tangent_gps_norm, tangent_lidar_norm)\n",
    "    dot_product = np.clip(dot_product, -1.0, 1.0)\n",
    "    angle_radians = np.arccos(dot_product)\n",
    "    sign = np.sign(tangent_gps[0] * tangent_lidar[1] - tangent_gps[1] * tangent_lidar[0])\n",
    "    angle_radians = sign * angle_radians\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "    print(angle_degrees)\n",
    "    angle_degrees = -1*angle_degrees + add_offset\n",
    "    \n",
    "    return angle_degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "26d15d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapbox_trajectories(traj: list, filename1: str):\n",
    "    def lat_lng_bounds(latitudes, longitudes):\n",
    "        min_lat, max_lat = min(latitudes), max(latitudes)\n",
    "        min_lng, max_lng = min(longitudes), max(longitudes)\n",
    "        return min_lat, min_lng, max_lat, max_lng\n",
    "\n",
    "    def map_center_and_zoom(min_lat, min_lng, max_lat, max_lng):\n",
    "        center_lat = (min_lat + max_lat) / 2\n",
    "        center_lng = (min_lng + max_lng) / 2\n",
    "\n",
    "        lat_diff = max_lat - min_lat\n",
    "        lng_diff = max_lng - min_lng\n",
    "\n",
    "        zoom_lat = math.log(180 / lat_diff) / math.log(2)\n",
    "        zoom_lng = math.log(360 / lng_diff) / math.log(2)\n",
    "\n",
    "        zoom = min(zoom_lat, zoom_lng)\n",
    "        return center_lat, center_lng, zoom\n",
    "\n",
    "    traces = []\n",
    "    all_latitudes = []\n",
    "    all_longitudes = []\n",
    "\n",
    "    for t in traj:\n",
    "        name, mat = t\n",
    "        latitudes = mat[:, 0]\n",
    "        longitudes = mat[:, 1]\n",
    "        all_latitudes.extend(latitudes)\n",
    "        all_longitudes.extend(longitudes)\n",
    "        trace = go.Scattermapbox(\n",
    "            lon=longitudes,\n",
    "            lat=latitudes,\n",
    "            mode='markers',\n",
    "            marker=go.scattermapbox.Marker(size=8),\n",
    "            name=name\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    min_lat, min_lng, max_lat, max_lng = lat_lng_bounds(all_latitudes, all_longitudes)\n",
    "    center_lat, center_lng, zoom = map_center_and_zoom(min_lat, min_lng, max_lat, max_lng)\n",
    "\n",
    "    fig = go.Figure(traces)\n",
    "\n",
    "    fig.update_layout(\n",
    "        mapbox=dict(\n",
    "            # Replace with your Mapbox access token\n",
    "            accesstoken='pk.eyJ1IjoiaGFyaXNiaW55b3VzYWYiLCJhIjoiY2xsMXpkcGNoMDlqZDNrcDZrbjdrNHZnNyJ9.nXLa2DYKp8vNfzxbJRokAQ',\n",
    "            center=dict(lat=center_lat, lon=center_lng),\n",
    "            zoom=zoom,\n",
    "            style='satellite'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        legend=dict(x=0, y=1, bgcolor='rgba(255, 255, 255, 0.5)')\n",
    "    )\n",
    "    pio.write_html(fig, file=filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "994fb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(gps_data, poses, translations, params, output_file):\n",
    "    frame_idx = params['frame_index']\n",
    "    add_offset = params['offset']\n",
    "    rotate_via = params['use_heading_from']\n",
    "#     print(rotate_via)\n",
    "    manual_heading = params['manual_heading']\n",
    "    \n",
    "    transformed_poses = []\n",
    "    transformed_translations = []\n",
    "    \n",
    "    if rotate_via == 'gps':\n",
    "        angle = compute_rotation_based_on_gps(gps_data, translations, frame_idx, add_offset)\n",
    "#         print(angle)\n",
    "        r_matrix = rot_matrix(angle)\n",
    "#         print(r_matrix)\n",
    "    elif rotate_via == 'manual':\n",
    "        r_matrix = rot_matrix(manual_heading)\n",
    "        \n",
    "    for pose in poses:\n",
    "        rotated_pose = np.dot(r_matrix, pose)\n",
    "        translation = rotated_pose[:3, 3]\n",
    "        transformed_poses.append(rotated_pose)\n",
    "        transformed_translations.append(translation)\n",
    "        \n",
    "    tf_poses = np.array(transformed_poses)\n",
    "    tf_translations = np.array(transformed_translations)\n",
    "    np.save(output_file, tf_poses)\n",
    "    print(f\"Transformed poses saved at: {output_file}\")\n",
    "    return tf_poses, tf_translations\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0b406e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(gps, odom, output_file):\n",
    "    local_gps, offset = convert_global_to_local(gps)\n",
    "    global_odom = convert_local_to_global(odom, offset)\n",
    "#     # Debug statements\n",
    "#     print(\"Local GPS data:\")\n",
    "#     print(local_gps)\n",
    "#     print(\"Global Odom data:\")\n",
    "#     print(global_odom)\n",
    "    trajs = [('GPS', gps[:, :2]), ('Lidar', global_odom)]\n",
    "    mapbox_trajectories(trajs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9c579fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_plot_trajectories(params):\n",
    "    gps_file = os.path.join(data_dir, 'gnss.json')\n",
    "\n",
    "    for root, dirs, files in os.walk(odom_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                odom_file = os.path.join(root, file)\n",
    "                break\n",
    "    \n",
    "    trajectories = os.path.join(odom_dir, 'trajectories')\n",
    "    os.makedirs(trajectories, exist_ok=True)\n",
    "    \n",
    "    gps_data = read_gps(gps_file)\n",
    "    poses, translations = load_states(odom_file)\n",
    "    print(translations[:5])\n",
    "    traj_file = os.path.join(trajectories, 'transformed_poses.npy')\n",
    "    local_gps, _ = convert_global_to_local(gps_data)\n",
    "    tf_poses, tf_translations = apply_transformation(gps_data, poses, translations, params, traj_file)\n",
    "    print(tf_translations[:5])\n",
    "    org_plot_file = os.path.join(trajectories, 'lidar_gps_raw.html')\n",
    "    plot_trajectories(gps_data, translations, org_plot_file)\n",
    "    print(f\"Raw trajectories plot saved at: {org_plot_file}\")\n",
    "    \n",
    "    tf_plot_file = os.path.join(trajectories, 'lidar_gps_transformed.html')\n",
    "    plot_trajectories(gps_data, tf_translations, tf_plot_file)\n",
    "    print(f\"Transformed trajectories plot saved at: {tf_plot_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3b35a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'frame_index':-1,\n",
    "         'offset':0,\n",
    "         'use_heading_from':'gps',\n",
    "         'manual_heading': 0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e36c3134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [ 0.01077481  1.16174201 -0.01573739]\n",
      " [ 0.03017555  2.88316786 -0.03786862]\n",
      " [ 0.0539516   4.37269494 -0.04436926]\n",
      " [ 0.05498653  5.4116076  -0.04029995]]\n",
      "-0.0\n",
      "Transformed poses saved at: /home/ty/Downloads/AAI/Bags/hamburg/odometry/trajectories/transformed_poses.npy\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.01077481  1.16174201 -0.01573739]\n",
      " [ 0.03017555  2.88316786 -0.03786862]\n",
      " [ 0.0539516   4.37269494 -0.04436926]\n",
      " [ 0.05498653  5.4116076  -0.04029995]]\n",
      "Raw trajectories plot saved at: /home/ty/Downloads/AAI/Bags/hamburg/odometry/trajectories/lidar_gps_raw.html\n",
      "Transformed trajectories plot saved at: /home/ty/Downloads/AAI/Bags/hamburg/odometry/trajectories/lidar_gps_transformed.html\n"
     ]
    }
   ],
   "source": [
    "transform_and_plot_trajectories(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa362f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kiss-icp-env",
   "language": "python",
   "name": "kiss-icp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
